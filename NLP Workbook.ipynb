{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38241feb",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983a46de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Secretariat', 'NNP'), ('is', 'VBZ'), ('expected', 'VBN'), ('to', 'TO'), ('race', 'NN'), ('tomorrow', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Parts of Speech Tagging using NLTK\n",
    "import nltk\n",
    "# download required nltk packages\n",
    "# required for tokenization\n",
    "nltk.download('punkt')\n",
    "# required for parts of speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# input text\n",
    "sentence = \"Secretariat is expected to race tomorrow\"\n",
    "# tokene into words\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "# parts of speech tagging\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "# print tagged tokens\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e8eb2",
   "metadata": {},
   "source": [
    "### Context Free Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6647f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('girl', 'NN'), ('is', 'VBZ'), ('laughing', 'VBG')]\n",
      "(S (NP (Det The) (NP (N girl))) (VP (Aux is) (VP (V laughing))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "# When a chart parser begins parsing a text, it creates a new (empty) chart, spanning the text.\n",
    "# It then incrementally adds new edges to the chart.\n",
    "# A set of \"chart rules\" specifies the conditions under which new edges should be added to the chart.\n",
    "# Once the chart reaches a stage where none of the chart rules adds any new edges, parsing is complete.\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP | Aux VP | V\n",
    "NP -> Det NP | N | Adj NP | Adj\n",
    "N -> \"girl\" | \"boy\"\n",
    "Det -> \"The\"\n",
    "Aux -> \"is\"\n",
    "V -> \"laughing\" | \"playing\"\n",
    "Adj -> \"laughing\" | \"well\"\n",
    "\"\"\")\n",
    "statement = nltk.word_tokenize(\"The girl is laughing\")\n",
    "print(nltk.pos_tag(statement))\n",
    "total_trees = 0\n",
    "rd_parser = nltk.ChartParser(grammar1)\n",
    "for tree in rd_parser.parse(statement):\n",
    "    total_trees = total_trees + 1\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3f110",
   "metadata": {},
   "source": [
    "### Probabilistic Parsing Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2ec554",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Productions for NP do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5064/3721374822.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Define the grammar rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m grammar = nltk.PCFG.fromstring(\"\"\"\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNP\u001b[0m \u001b[0mVP\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mVP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mV\u001b[0m \u001b[0mPP\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TFCert\\venv\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mfromstring\u001b[1;34m(cls, input, encoding)\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandard_nonterm_parser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilistic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         )\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproductions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TFCert\\venv\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, start, productions, calculate_leftcorners)\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mPCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Productions for %r do not sum to 1\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Productions for NP do not sum to 1"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Define the grammar rules\n",
    "grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    VP -> V PP [0.4]\n",
    "    VP -> V NP [0.6]\n",
    "    PP -> P NP [1.0]\n",
    "    NP -> V NP [0.1]\n",
    "    NP -> NP PP [0.3]\n",
    "    NP -> N [0.3]\n",
    "    N -> 'visit' [0.3]\n",
    "    V -> 'visit' [0.6]\n",
    "    N -> 'Goa' [0.3]\n",
    "    N -> 'She' [0.5]\n",
    "    V -> 'loves' [1]\n",
    "    P -> 'to' [1]\n",
    "\"\"\")\n",
    "\n",
    "# Create a Probabilistic ChartParser with the grammar\n",
    "parser = nltk.ViterbiParser(grammar)\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"She loves to visit Goa\".split()\n",
    "\n",
    "# Parse the sentence\n",
    "for tree in parser.parse(sentence):\n",
    "    # Print the parse tree\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a878d",
   "metadata": {},
   "source": [
    "### n_gram calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f06dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "| Unigram   |\n",
      "+===========+\n",
      "| s         |\n",
      "+-----------+\n",
      "| Life      |\n",
      "+-----------+\n",
      "| should    |\n",
      "+-----------+\n",
      "| be        |\n",
      "+-----------+\n",
      "| great     |\n",
      "+-----------+\n",
      "| rather    |\n",
      "+-----------+\n",
      "| than      |\n",
      "+-----------+\n",
      "| long      |\n",
      "+-----------+\n",
      "| e         |\n",
      "+-----------+\n",
      "+--------------+\n",
      "| Bigram       |\n",
      "+==============+\n",
      "| s Life       |\n",
      "+--------------+\n",
      "| Life should  |\n",
      "+--------------+\n",
      "| should be    |\n",
      "+--------------+\n",
      "| be great     |\n",
      "+--------------+\n",
      "| great rather |\n",
      "+--------------+\n",
      "| rather than  |\n",
      "+--------------+\n",
      "| than long    |\n",
      "+--------------+\n",
      "| long e       |\n",
      "+--------------+\n",
      "+-------------------+\n",
      "| Trigram           |\n",
      "+===================+\n",
      "| s Life should     |\n",
      "+-------------------+\n",
      "| Life should be    |\n",
      "+-------------------+\n",
      "| should be great   |\n",
      "+-------------------+\n",
      "| be great rather   |\n",
      "+-------------------+\n",
      "| great rather than |\n",
      "+-------------------+\n",
      "| rather than long  |\n",
      "+-------------------+\n",
      "| than long e       |\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def ngrams(s, n=2, i=0):\n",
    "    while len(s[i:i+n]) == n:\n",
    "        yield s[i:i+n]\n",
    "        i += 1\n",
    "\n",
    "\n",
    "txt = 's Life should be great rather than long e'\n",
    "unigram = ngrams(txt.split(), n=1)\n",
    "bigram = ngrams(txt.split(), n=2)\n",
    "trigram = ngrams(txt.split(), n=3)\n",
    "# Convert ngrams to lists\n",
    "unigram_list = list(unigram)\n",
    "bigram_list = list(bigram)\n",
    "trigram_list = list(trigram)\n",
    "# Format output as table using tabulate\n",
    "print(tabulate([[' '.join(ngram)] for ngram in unigram_list], headers=['Unigram'], tablefmt='grid'))\n",
    "print(tabulate([[' '.join(ngram)] for ngram in bigram_list], headers=['Bigram'], tablefmt='grid'))\n",
    "print(tabulate([[' '.join(ngram)] for ngram in trigram_list], headers=['Trigram'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731a0dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "| Trigram                     |\n",
      "+=============================+\n",
      "| s Natural Language          |\n",
      "+-----------------------------+\n",
      "| Natural Language processing |\n",
      "+-----------------------------+\n",
      "| Language processing is      |\n",
      "+-----------------------------+\n",
      "| processing is very          |\n",
      "+-----------------------------+\n",
      "| is very interesting         |\n",
      "+-----------------------------+\n",
      "| very interesting though     |\n",
      "+-----------------------------+\n",
      "| interesting though not      |\n",
      "+-----------------------------+\n",
      "| though not easy             |\n",
      "+-----------------------------+\n",
      "| not easy e                  |\n",
      "+-----------------------------+\n",
      "Number of trigrams: 9\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def ngrams(s, n=2, i=0):\n",
    "    while len(s[i:i+n]) == n:\n",
    "        yield s[i:i+n]\n",
    "        i += 1\n",
    "\n",
    "txt = 's Natural Language processing is very interesting though not easy e'\n",
    "trigram = ngrams(txt.split(), n=3)\n",
    "# Convert ngrams to lists\n",
    "trigram_list = list(trigram)\n",
    "# Count the number of trigrams\n",
    "trigram_count = len(trigram_list)\n",
    "# Format output as table using tabulate\n",
    "print(tabulate([[' '.join(ngram)] for ngram in trigram_list], headers=['Trigram'], tablefmt='grid'))\n",
    "print(f\"Number of trigrams: {trigram_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc74ade",
   "metadata": {},
   "source": [
    "### Estimating bigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16fa3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus: s The Arabian knights e s These are the fairy tales of the east e s The stories of the Arabian knights are translated in many lang\n",
    "# Test : s The Arabian knights are the fairy tales of the east e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6a9000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter corpus = \n",
      "  @media print {\n",
      "    .ms-editor-squiggles-container {\n",
      "      display:none !important;\n",
      "    }\n",
      "  }\n",
      "  .ms-editor-squiggles-container {\n",
      "    all: initial;\n",
      "  }s The Arabian knights e s These are the fairy tales of the east e s The stories of the Arabian knights are translated in many lang\n",
      "Preprocessed Data corpus = \n",
      " s the arabian knights e s these are the fairy tales of the east e s the stories of the arabian knights are translated in many lang\n",
      "Tokens in the corpus = \n",
      " ['of', 'tales', 'stories', 'the', 'these', 'e', 'lang', 'arabian', 's', 'translated', 'in', 'knights', 'east', 'many', 'are', 'fairy']\n",
      "Frequency of each tokens = \n",
      "s \t: 3\n",
      "the \t: 5\n",
      "arabian \t: 2\n",
      "knights \t: 2\n",
      "e \t: 2\n",
      "these \t: 1\n",
      "are \t: 2\n",
      "fairy \t: 1\n",
      "tales \t: 1\n",
      "of \t: 2\n",
      "east \t: 1\n",
      "stories \t: 1\n",
      "translated \t: 1\n",
      "in \t: 1\n",
      "many \t: 1\n",
      "lang \t: 1\n",
      "N-grams generated (Here n is 2) = \n",
      "['s', 'the']\n",
      "['the', 'arabian']\n",
      "['arabian', 'knights']\n",
      "['knights', 'e']\n",
      "['e', 's']\n",
      "['s', 'these']\n",
      "['these', 'are']\n",
      "['are', 'the']\n",
      "['the', 'fairy']\n",
      "['fairy', 'tales']\n",
      "['tales', 'of']\n",
      "['of', 'the']\n",
      "['the', 'east']\n",
      "['east', 'e']\n",
      "['e', 's']\n",
      "['s', 'the']\n",
      "['the', 'stories']\n",
      "['stories', 'of']\n",
      "['of', 'the']\n",
      "['the', 'arabian']\n",
      "['arabian', 'knights']\n",
      "['knights', 'are']\n",
      "['are', 'translated']\n",
      "['translated', 'in']\n",
      "['in', 'many']\n",
      "['many', 'lang']\n",
      "Frequency of n-grams = \n",
      "s the : 2\n",
      "the arabian : 2\n",
      "arabian knights : 2\n",
      "knights e : 1\n",
      "e s : 2\n",
      "s these : 1\n",
      "these are : 1\n",
      "are the : 1\n",
      "the fairy : 1\n",
      "fairy tales : 1\n",
      "tales of : 1\n",
      "of the : 2\n",
      "the east : 1\n",
      "east e : 1\n",
      "the stories : 1\n",
      "stories of : 1\n",
      "knights are : 1\n",
      "are translated : 1\n",
      "translated in : 1\n",
      "in many : 1\n",
      "many lang : 1\n",
      "Probability table = \n",
      "\n",
      "\tof\ttales\tstories\tthe\tthese\te\tlang\tarabian\ts\ttranslated\tin\tknights\teast\tmany\tare\tfairy\t\n",
      "\n",
      "of\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "tales\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "stories\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "the\t0.0\t0.0\t0.2\t0.0\t0.0\t0.0\t0.0\t0.4\t0.0\t0.0\t0.0\t0.0\t0.2\t0.0\t0.0\t0.2\t\n",
      "\n",
      "these\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t\n",
      "\n",
      "e\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "lang\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "arabian\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "s\t0.0\t0.0\t0.0\t0.667\t0.333\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "translated\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "in\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t\n",
      "\n",
      "knights\t0.0\t0.0\t0.0\t0.0\t0.0\t0.5\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.5\t0.0\t\n",
      "\n",
      "east\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "many\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "are\t0.0\t0.0\t0.0\t0.5\t0.0\t0.0\t0.0\t0.0\t0.0\t0.5\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "fairy\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t\n",
      "\n",
      "Enter Text = \n",
      "\n",
      "  @media print {\n",
      "    .ms-editor-squiggles-container {\n",
      "      display:none !important;\n",
      "    }\n",
      "  }\n",
      "  .ms-editor-squiggles-container {\n",
      "    all: initial;\n",
      "  }s The Arabian knights are the fairy tales of the east e\n",
      "Preprocessed Text = \n",
      " s the arabian knights are the fairy tales of the east e \n",
      "\n",
      "Tokens Generated = \n",
      " ['s', 'the', 'arabian', 'knights', 'are', 'the', 'fairy', 'tales', 'of', 'the', 'east', 'e'] \n",
      "\n",
      "N-grams Generated = \n",
      "=  [['s', 'the'], ['the', 'arabian'], ['arabian', 'knights'], ['knights', 'are'], ['are', 'the'], ['the', 'fairy'], ['fairy', 'tales'], ['tales', 'of'], ['of', 'the'], ['the', 'east'], ['east', 'e']]\n",
      "\u001b[1mCalculate bigram probability\u001b[0m\n",
      "\n",
      "P('s the')\t=  0.667\n",
      "P('the arabian')\t=  0.4\n",
      "P('arabian knights')\t=  1.0\n",
      "P('knights are')\t=  0.5\n",
      "P('are the')\t=  0.5\n",
      "P('the fairy')\t=  0.2\n",
      "P('fairy tales')\t=  1.0\n",
      "P('tales of')\t=  1.0\n",
      "P('of the')\t=  1.0\n",
      "P('the east')\t=  0.2\n",
      "P('east e')\t=  1.0\n",
      "\n",
      "\u001b[1mCalculate Probability of the sentence\u001b[0m\n",
      "P('s The Arabian knights are the fairy tales of the east e') \n",
      "= P('s the') * P('the arabian') * P('arabian knights') * P('knights are') * P('are the') * P('the fairy') * P('fairy tales') * P('tales of') * P('of the') * P('the east') * P('east e')\n",
      "= 0.667 * 0.4 * 1.0 * 0.5 * 0.5 * 0.2 * 1.0 * 1.0 * 1.0 * 0.2 * 1.0 \n",
      "= 0.0026680000000000007\n",
      "\n",
      "\u001b[1mProbability('s The Arabian knights are the fairy tales of the east e') = 0.00267\n"
     ]
    }
   ],
   "source": [
    "d=input(\"Enter corpus = \")\n",
    "\n",
    "def preprocess(d):\n",
    "    d=d.lower()\n",
    "    return d\n",
    "d=preprocess(d)\n",
    "print(\"Preprocessed Data corpus = \\n\",d)\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "def generate_tokens(d):\n",
    "    tokens = word_tokenize(d)\n",
    "    return tokens\n",
    "tokens=generate_tokens(d)\n",
    "distinct_tokens = list(set(sorted(tokens)))\n",
    "print(\"Tokens in the corpus = \\n\",distinct_tokens)\n",
    "\n",
    "\n",
    "def generate_tokens_freq(tokens):\n",
    "    dct={}\n",
    "    for i in tokens:\n",
    "        dct[i]=0\n",
    "    for i in tokens:\n",
    "        dct[i]+=1\n",
    "    return dct\n",
    "dct=generate_tokens_freq(tokens)\n",
    "print(\"Frequency of each tokens = \")\n",
    "for i in dct.items():\n",
    "    print(i[0],\"\\t:\" , i[1])\n",
    "\n",
    "\n",
    "def generate_ngrams(tokens,k):\n",
    "    l=[]\n",
    "    i=0\n",
    "    while(i<len(tokens)):\n",
    "        l.append(tokens[i:i+k])\n",
    "        i=i+1\n",
    "    l=l[:-1]\n",
    "    return l\n",
    "bigram = generate_ngrams(tokens,2)\n",
    "print(\"N-grams generated (Here n is 2) = \")\n",
    "for i in bigram:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "def generate_ngram_freq(bigram):\n",
    "    dct1={}\n",
    "    for i in bigram:\n",
    "        st=\" \".join(i)\n",
    "        dct1[st]=0\n",
    "    for i in bigram:\n",
    "        st=\" \".join(i)\n",
    "        dct1[st]+=1\n",
    "    return dct1\n",
    "dct1=generate_ngram_freq(bigram)\n",
    "print(\"Frequency of n-grams = \")\n",
    "for i in dct1.items():\n",
    "    print(i[0], \":\", i[1])\n",
    "    \n",
    "\n",
    "def find1(s,dct1):\n",
    "    try:\n",
    "        return dct1[s]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def print_probability_table(distinct_tokens,dct,dct1):\n",
    "    n=len(distinct_tokens)\n",
    "    l=[[]*n for i in range(n)]\n",
    "    for i in range(n):\n",
    "        denominator = dct[distinct_tokens[i]]\n",
    "        for j in range(n):\n",
    "            numerator = find1(distinct_tokens[i]+\" \"+distinct_tokens[j],dct1)\n",
    "            l[i].append(float(\"{:.3f}\".format(numerator/denominator)))\n",
    "    return l\n",
    "\n",
    "\n",
    "print(\"Probability table = \\n\")\n",
    "probability_table=print_probability_table(distinct_tokens,dct,dct1)\n",
    "n=len(distinct_tokens)\n",
    "print(\"\\t\", end=\"\")\n",
    "for i in range(n):\n",
    "    print(distinct_tokens[i],end=\"\\t\")\n",
    "print(\"\\n\")\n",
    "for i in range(n):\n",
    "    print(distinct_tokens[i],end=\"\\t\")\n",
    "    for j in range(n):\n",
    "        print(probability_table[i][j],end=\"\\t\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "text = input(\"Enter Text = \\n\")\n",
    "p = preprocess(text)\n",
    "print(\"Preprocessed Text = \\n\",p,\"\\n\")\n",
    "t = generate_tokens(p)\n",
    "print(\"Tokens Generated = \\n\",t,\"\\n\")\n",
    "n = generate_ngrams(t,2)\n",
    "print(\"N-grams Generated = \\n= \",n)\n",
    "\n",
    "print('\\033[1m'+\"Calculate bigram probability\"+'\\033[0m\\n')\n",
    "s=1\n",
    "dct2={}\n",
    "for i in n:\n",
    "    dct2[\" \".join(i)]=0\n",
    "for i in n:\n",
    "    k=distinct_tokens.index(i[0])\n",
    "    m=distinct_tokens.index(i[1])\n",
    "    dct2[\" \".join(i)]=probability_table[k][m]\n",
    "    print(\"P('{}')\\t= \".format(' '.join(i)),probability_table[k][m])\n",
    "    s*=probability_table[k][m]\n",
    "print(\"\\n\"+'\\033[1m'+ \"Calculate Probability of the sentence\"+'\\033[0m')\n",
    "print(f\"P('{text}') \\n= \",end=\"\")\n",
    "x=dct2.popitem()\n",
    "for i in dct2:\n",
    "    print(f\"P('{i}')\", end=\" * \")\n",
    "print(f\"P('{x[0]}')\\n= \", end='')\n",
    "for i in dct2:\n",
    "    print(dct2[i], end=\" * \")\n",
    "print(x[1],\"\\n=\",s)\n",
    "\n",
    "print(\"\\n\"+'\\033[1m'+f\"Probability('{text}') = \"+\"{:.5f}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e4aa0",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0789b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4.778522998487945\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(sentence):\n",
    "    # Convert the sentence to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Create a dictionary to store the counts of each alphabet\n",
    "    counts = {}\n",
    "    for char in sentence:\n",
    "        if char in counts:\n",
    "            counts[char] += 1\n",
    "        else:\n",
    "            counts[char] = 1\n",
    "    # Calculate the total number of characters in the sentence\n",
    "    total_chars = len(sentence)\n",
    "    # Calculate the probability of each character\n",
    "    probabilities = {}\n",
    "    for char, count in counts.items():\n",
    "        probabilities[char] = count / total_chars\n",
    "    # Calculate the perplexity\n",
    "    perplexity = math.pow(2, -sum([probabilities[char] * math.log2(probabilities[char]) for char in counts]))\n",
    "    return perplexity\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"aabbccddeeAABBC\"\n",
    "# Calculate the perplexity of the sentence\n",
    "perplexity = calculate_perplexity(sentence)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420b1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4.685532271563225\n"
     ]
    }
   ],
   "source": [
    "# in general Perplexity = 2^(- sum(p(x) * log2(p(x))) / N)\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(probabilities):\n",
    "    # probabilities is a list or dictionary of probabilities for each event\n",
    "    # sum of probabilities should be 1\n",
    "    # Calculate the sum of p(x) * log2(p(x))\n",
    "    sum_log_prob = sum([prob * math.log2(prob) for prob in probabilities])\n",
    "    # Calculate the perplexity\n",
    "    perplexity = 2 ** (-sum_log_prob)\n",
    "    return perplexity\n",
    "# Example probabilities for events\n",
    "probabilities = [0.2, 0.3, 0.1, 0.15, 0.25]\n",
    "# Calculate the perplexity\n",
    "perplexity = calculate_perplexity(probabilities)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaca5a6",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7498dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            2\n",
      "Healthy: 0.30000 0.08400 0.00588\n",
      "Fever: 0.04000 0.02700 0.01512\n",
      "The steps of states are Healthy Healthy Fever with highest probability of 0.01512\n"
     ]
    }
   ],
   "source": [
    "observations = (\"normal\", \"cold\", \"dizzy\")\n",
    "states = (\"Healthy\", \"Fever\")\n",
    "start_p = {\"Healthy\": 0.6, \"Fever\": 0.4}\n",
    "trans_p = {\n",
    "\"Healthy\": {\"Healthy\": 0.7, \"Fever\": 0.3},\n",
    "\"Fever\": {\"Healthy\": 0.4, \"Fever\": 0.6},\n",
    "}\n",
    "emit_p = {\n",
    "\"Healthy\": {\"normal\": 0.5, \"cold\": 0.4, \"dizzy\": 0.1},\n",
    "\"Fever\": {\"normal\": 0.1, \"cold\": 0.3, \"dizzy\": 0.6},\n",
    "}\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
    "        \n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "        \n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "    \n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "        \n",
    "    print (\"The steps of states are \" + \" \".join(opt) + \" with highest probability of %s\" % max_prob)\n",
    "    \n",
    "def dptable(V):\n",
    "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
    "viterbi_algorithm(observations, states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649e54e",
   "metadata": {},
   "source": [
    "### Hidden Markov Models_Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a99fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            2\n",
      "Healthy: 0.30000 0.08400 0.00588\n",
      "Fever: 0.04000 0.02700 0.01512\n",
      "The steps of states are Healthy Healthy Fever with highest probability of 0.01512\n"
     ]
    }
   ],
   "source": [
    "observations = (\"normal\", \"cold\", \"dizzy\")\n",
    "states = (\"Healthy\", \"Fever\")\n",
    "start_p = {\"Healthy\": 0.6, \"Fever\": 0.4}\n",
    "trans_p = {\n",
    "\"Healthy\": {\"Healthy\": 0.7, \"Fever\": 0.3},\n",
    "\"Fever\": {\"Healthy\": 0.4, \"Fever\": 0.6},\n",
    "}\n",
    "emit_p = {\n",
    "\"Healthy\": {\"normal\": 0.5, \"cold\": 0.4, \"dizzy\": 0.1},\n",
    "\"Fever\": {\"normal\": 0.1, \"cold\": 0.3, \"dizzy\": 0.6},\n",
    "}\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "        \n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "    \n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "    print (\"The steps of states are \" + \" \".join(opt) + \" with highest probability of %s\" % max_prob)\n",
    "\n",
    "def dptable(V):\n",
    "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
    "viterbi_algorithm(observations, states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13bb9e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1\n",
      "Low: 0.12000 0.05760\n",
      "High: 0.48000 0.15360\n",
      "The steps of states are High High with highest probability of 0.15360000000000001\n"
     ]
    }
   ],
   "source": [
    "observations = (\"Dry\", \"Rain\")\n",
    "states = (\"Low\", \"High\")\n",
    "start_p = {\"Low\": 0.3, \"High\": 0.8}\n",
    "trans_p = {\n",
    "\"Low\": {\"Low\": 0.6, \"High\": 0.7},\n",
    "\"High\": {\"Low\": 0.2, \"High\": 0.8},\n",
    "}\n",
    "emit_p = {\n",
    "\"Low\": {\"Dry\": 0.4, \"Rain\": 0.6},\n",
    "\"High\": {\"Dry\": 0.6, \"Rain\": 0.4},\n",
    "}\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
    "        \n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "        \n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "    \n",
    "    \n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "        \n",
    "    print (\"The steps of states are \" + \" \".join(opt) + \" with highest probability of %s\" % max_prob)\n",
    "\n",
    "\n",
    "def dptable(V):\n",
    "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
    "viterbi_algorithm(observations, states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c90cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            2            3\n",
      "VB: 0.00000 0.00005 0.00000 0.00000\n",
      "TO: 0.00000 0.00000 0.00000 0.00000\n",
      "NN: 0.00000 0.00000 0.00000 0.00000\n",
      "PPSS: 0.02479 0.00000 0.00000 0.00000\n",
      "The steps of states are PPSS VB TO VB with highest probability of 1.6636317629400003e-10\n"
     ]
    }
   ],
   "source": [
    "observations = (\"I\", \"want\", \"to\",\"race\")\n",
    "states = (\"VB\", \"TO\",\"NN\", \"PPSS\")\n",
    "start_p = {\"VB\": 0.019, \"TO\": 0.0043,\"NN\": 0.041, \"PPSS\": 0.067}\n",
    "trans_p = {\n",
    "\"VB\": {\"VB\": 0.0038, \"TO\": 0.035,\"NN\": 0.047, \"PPSS\": 0.007},\n",
    "\"TO\": {\"VB\": 0.83, \"TO\": 0.0,\"NN\": 0.0047, \"PPSS\": 0.0},\n",
    "\"NN\": {\"VB\": 0.0040, \"TO\": 0.0016,\"NN\": 0.087, \"PPSS\": 0.0045},\n",
    "\"PPSS\": {\"VB\": 0.23, \"TO\": 0.00079,\"NN\": 0.0012, \"PPSS\": 0.00014},\n",
    "}\n",
    "emit_p = {\n",
    "\"VB\": {\"I\": 0.0, \"want\": 0.0093, \"to\": 0.0,\"race\": 0.00012},\n",
    "\"TO\": {\"I\": 0.0, \"want\": 0.0, \"to\": 0.9,\"race\": 0.0},\n",
    "\"NN\": {\"I\": 0.0, \"want\": 0.000054, \"to\": 0.0,\"race\": 0.00057},\n",
    "\"PPSS\": {\"I\": 0.37, \"want\": 0.0, \"to\": 0.0,\"race\": 0.0},\n",
    "}\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
    "\n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "                    \n",
    "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "        \n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "    \n",
    "    \n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "    print (\"The steps of states are \" + \" \".join(opt) + \" with highest probability of %s\" % max_prob)\n",
    "def dptable(V):\n",
    "    \n",
    "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
    "viterbi_algorithm(observations, states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637870b",
   "metadata": {},
   "source": [
    "### CKY Parsinng: Chomsky Normal Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d76ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Tree:\n",
      "                  S                 \n",
      "      ____________|______            \n",
      "     |                   VP         \n",
      "     |             ______|___        \n",
      "     NP           |          NP     \n",
      "  ___|____        |       ___|___    \n",
      "Det       N       V     Det      N  \n",
      " |        |       |      |       |   \n",
      "The     flight includes  a      meal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "# Define the grammar rules\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> Det N\n",
    "N -> \"meal\" | \"flight\"\n",
    "Det -> \"The\" | \"a\"\n",
    "V -> \"includes\"\n",
    "\"\"\")\n",
    "\n",
    "# Define the example sentence\n",
    "sentence = \"The flight includes a meal\"\n",
    "\n",
    "# Step 1: Tokenize the sentence\n",
    "tokens = sentence.split()\n",
    "\n",
    "# Step 2: Initialize the parse table\n",
    "n = len(tokens)\n",
    "parse_table = [[[] for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "# Step 3: Fill in the parse table bottom-up\n",
    "for j in range(n):\n",
    "    for i in range(j, -1, -1):\n",
    "        if i == j:\n",
    "            # Case 1: Single word production\n",
    "            word = tokens[i]\n",
    "            for prod in grammar.productions():\n",
    "                if prod.rhs() == (word,):\n",
    "                    parse_table[i][j].append(Tree(prod.lhs(), [word]))\n",
    "        else:\n",
    "            # Case 2: Combine two non-terminals\n",
    "            for k in range(i, j):\n",
    "                for B in parse_table[i][k]:\n",
    "                    for C in parse_table[k+1][j]:\n",
    "                        for prod in grammar.productions():\n",
    "                            if prod.rhs() == (B.label(), C.label()):\n",
    "                                parse_table[i][j].append(Tree(prod.lhs(), [B, C]))\n",
    "\n",
    "# Step 4: Extract the parse tree from the parse table\n",
    "parse_tree = parse_table[0][n-1]\n",
    "# Step 5: Convert Nonterminal to Tree and print the parse tree\n",
    "if len(parse_tree) > 0:\n",
    "    print(\"Parse Tree:\")\n",
    "    Tree.fromstring(str(parse_tree[0])).pretty_print()\n",
    "else:\n",
    "    print(\"No valid parse tree found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539ebd4",
   "metadata": {},
   "source": [
    "### HMM Depndency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac422825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I saw the cat on the mat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"15c79b57ef44475fb98aa9a334e4e9bc-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">mat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15c79b57ef44475fb98aa9a334e4e9bc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the small English language model of Spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Define a simple sentence\n",
    "sentence = \"I saw the cat on the mat\"\n",
    "# Display the dependency parser tree for the sentence\n",
    "print(\"Sentence:\", sentence)\n",
    "doc = nlp(sentence)\n",
    "spacy.displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70532881",
   "metadata": {},
   "source": [
    "### ARC EAGER PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cccb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| Stack           | Buffer          | Arcs                             | Valid Transitions                            |\n",
      "+=================+=================+==================================+==============================================+\n",
      "| []              | [0, 1, 2, 3, 4] | []                               | ['shift']                                    |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0]             | [1, 2, 3, 4]    | []                               | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1]          | [2, 3, 4]       | [(0, 1)]                         | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2]       | [3, 4]          | [(0, 1), (1, 2)]                 | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3]    | [4]             | [(0, 1), (1, 2), (2, 3)]         | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3, 4] | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3]    | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2]       | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1]          | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0]             | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "# Define a class for the Arc Eager parser\n",
    "class ArcEagerParser:\n",
    "    def __init__(self, sentence):\n",
    "        # Set the sentence and initialize the buffer and stack\n",
    "        self.sentence = sentence\n",
    "        self.buffer = list(range(len(sentence)))\n",
    "        self.stack = []\n",
    "        self.arcs = []\n",
    "    \n",
    "    # Define the valid transitions from the current parser state\n",
    "    def valid_transitions(self):\n",
    "        valid = []\n",
    "        if len(self.buffer) > 0:\n",
    "            valid.append('shift')\n",
    "        if len(self.stack) > 0:\n",
    "            valid.append('reduce')\n",
    "        if len(self.stack) > 0 and len(self.buffer) > 0:\n",
    "            valid.append('left_arc')\n",
    "            valid.append('right_arc')\n",
    "        return valid\n",
    "\n",
    "    # Define the apply transition method\n",
    "    def apply_transition(self, transition):\n",
    "        if transition == 'shift':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'reduce':\n",
    "            self.stack.pop()\n",
    "        elif transition == 'left_arc':\n",
    "            head = self.stack[-1]\n",
    "            dependent = self.buffer.pop(0)\n",
    "            self.arcs.append((head, dependent))\n",
    "            self.stack.append(dependent)\n",
    "        elif transition == 'right_arc':\n",
    "            head = self.stack.pop()\n",
    "            dependent = self.buffer.pop(0)\n",
    "            self.arcs.append((head, dependent))\n",
    "            self.stack.append(head)\n",
    "    \n",
    "    # Define the parse method\n",
    "    def parse(self):\n",
    "        table = []\n",
    "        while len(self.buffer) > 0 or len(self.stack) > 1:\n",
    "            valid_transitions = self.valid_transitions()\n",
    "            table.append([self.stack[:], self.buffer[:], self.arcs[:], valid_transitions])\n",
    "            if 'left_arc' in valid_transitions:\n",
    "                self.apply_transition('left_arc')\n",
    "            elif 'right_arc' in valid_transitions:\n",
    "                self.apply_transition('right_arc')\n",
    "            elif 'reduce' in valid_transitions:\n",
    "                self.apply_transition('reduce')\n",
    "            elif 'shift' in valid_transitions:\n",
    "                self.apply_transition('shift')\n",
    "            else:\n",
    "            # No valid transition found, break the loop\n",
    "                break\n",
    "        table.append([self.stack[:], self.buffer[:], self.arcs[:], self.valid_transitions()])\n",
    "        return table\n",
    "\n",
    "\n",
    "# Define a sample sentence to parse\n",
    "sentence = \"He sent her a letter\"\n",
    "# Tokenize the sentence\n",
    "tokens = sentence.split()\n",
    "# Create an instance of the ArcEagerParser\n",
    "parser = ArcEagerParser(tokens)\n",
    "# Parse the sentence and get the table with stack, buffer, and arcs at each step\n",
    "table = parser.parse()\n",
    "# Print the table with stack, buffer, and arcs at each step in a pretty format\n",
    "print(tabulate(table, headers=[\"Stack\", \"Buffer\", \"Arcs\", \"Valid Transitions\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897029d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| Stack           | Buffer          | Arcs                             | Valid Transitions                            |\n",
      "+=================+=================+==================================+==============================================+\n",
      "| []              | [0, 1, 2, 3, 4] | []                               | ['shift']                                    |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0]             | [1, 2, 3, 4]    | []                               | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1]          | [2, 3, 4]       | [(0, 1)]                         | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2]       | [3, 4]          | [(0, 1), (1, 2)]                 | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3]    | [4]             | [(0, 1), (1, 2), (2, 3)]         | ['shift', 'reduce', 'left_arc', 'right_arc'] |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3, 4] | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2, 3]    | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1, 2]       | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0, 1]          | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "| [0]             | []              | [(0, 1), (1, 2), (2, 3), (3, 4)] | ['reduce']                                   |\n",
      "+-----------------+-----------------+----------------------------------+----------------------------------------------+\n",
      "Sentence: She baught her a dress\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cfb3a201c04d4b44935b41da7be78275-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">baught</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">her</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">dress</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cfb3a201c04d4b44935b41da7be78275-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cfb3a201c04d4b44935b41da7be78275-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cfb3a201c04d4b44935b41da7be78275-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cfb3a201c04d4b44935b41da7be78275-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dative</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cfb3a201c04d4b44935b41da7be78275-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cfb3a201c04d4b44935b41da7be78275-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cfb3a201c04d4b44935b41da7be78275-0-3\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cfb3a201c04d4b44935b41da7be78275-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define a class for the Arc Eager parser\n",
    "class ArcEagerParser:\n",
    "    def __init__(self, sentence):\n",
    "        # Set the sentence and initialize the buffer and stack\n",
    "        self.sentence = sentence\n",
    "        self.buffer = list(range(len(sentence)))\n",
    "        self.stack = []\n",
    "        self.arcs = []\n",
    "\n",
    "    # Define the valid transitions from the current parser state\n",
    "    def valid_transitions(self):\n",
    "        valid = []\n",
    "        if len(self.buffer) > 0:\n",
    "            valid.append('shift')\n",
    "        if len(self.stack) > 0:\n",
    "            valid.append('reduce')\n",
    "        if len(self.stack) > 0 and len(self.buffer) > 0:\n",
    "            valid.append('left_arc')\n",
    "            valid.append('right_arc')\n",
    "        return valid\n",
    "\n",
    "    # Define the apply transition method\n",
    "    def apply_transition(self, transition):\n",
    "        if transition == 'shift':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'reduce':\n",
    "            self.stack.pop()\n",
    "        elif transition == 'left_arc':\n",
    "            head = self.stack[-1]\n",
    "            dependent = self.buffer.pop(0)\n",
    "            self.arcs.append((head, dependent))\n",
    "            self.stack.append(dependent)\n",
    "        elif transition == 'right_arc':\n",
    "            head = self.stack.pop()\n",
    "            dependent = self.buffer.pop(0)\n",
    "            self.arcs.append((head, dependent))\n",
    "            self.stack.append(head)\n",
    "    \n",
    "    \n",
    "    # Define the parse method\n",
    "    def parse(self):\n",
    "        table = []\n",
    "        while len(self.buffer) > 0 or len(self.stack) > 1:\n",
    "            valid_transitions = self.valid_transitions()\n",
    "            table.append([self.stack[:], self.buffer[:], self.arcs[:], valid_transitions])\n",
    "            if 'left_arc' in valid_transitions:\n",
    "                self.apply_transition('left_arc')\n",
    "            elif 'right_arc' in valid_transitions:\n",
    "                self.apply_transition('right_arc')\n",
    "            elif 'reduce' in valid_transitions:\n",
    "                self.apply_transition('reduce')\n",
    "            elif 'shift' in valid_transitions:\n",
    "                self.apply_transition('shift')\n",
    "            else:\n",
    "                # No valid transition found, break the loop\n",
    "                break\n",
    "        table.append([self.stack[:], self.buffer[:], self.arcs[:], self.valid_transitions()])\n",
    "        return table\n",
    "\n",
    "\n",
    "\n",
    "# Define a sample sentence to parse\n",
    "sentence = \"She baught her a dress\"\n",
    "# Tokenize the sentence\n",
    "tokens = sentence.split()\n",
    "# Create an instance of the ArcEagerParser\n",
    "parser = ArcEagerParser(tokens)\n",
    "# Parse the sentence and get the table with stack, buffer, and arcs at each step\n",
    "table = parser.parse()\n",
    "# Print the table with stack, buffer, and arcs at each step in a pretty format\n",
    "print(tabulate(table, headers=[\"Stack\", \"Buffer\", \"Arcs\", \"Valid Transitions\"], tablefmt=\"grid\"))\n",
    "import spacy\n",
    "# Load the small English language model of Spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Display the dependency parser tree for the sentence\n",
    "print(\"Sentence:\", sentence)\n",
    "doc = nlp(sentence)\n",
    "spacy.displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9347c6",
   "metadata": {},
   "source": [
    "### chu liu edmonds algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee72e8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'contract' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5064/573053939.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Call the chu_liu_edmonds function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mmst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchu_liu_edmonds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;31m# Print the minimum spanning tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minimum Spanning Tree:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5064/573053939.py\u001b[0m in \u001b[0;36mchu_liu_edmonds\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mcontracted_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontract\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontract\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mcontracted_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontract\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontract\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'contract' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def chu_liu_edmonds(graph):\n",
    "    n = max(max(u, v) for u, v, _ in graph) + 1 # Get the total number of nodes in the graph\n",
    "    parent = [-1] * n # Initialize parent array with -1 for all nodes\n",
    "    best_weight = [float('inf')] * n # Initialize best_weight array with positive infinity for all nodes\n",
    "    \n",
    "    # Step 1: Find the best incoming edge for each node\n",
    "    for u, v, weight in graph:\n",
    "        if weight < best_weight[v]:\n",
    "            parent[v] = u\n",
    "            best_weight[v] = weight\n",
    "    \n",
    "    # Step 2: Check for cycles\n",
    "    cycle = None\n",
    "    for u, v, weight in graph:\n",
    "        if best_weight[v] == float('inf'):\n",
    "            continue\n",
    "        if best_weight[v] > weight:\n",
    "            cycle = v\n",
    "            break\n",
    "    \n",
    "    if cycle is not None:\n",
    "        # Step 3: Contract the cycle into a single node\n",
    "        contract = [cycle]\n",
    "        visited = [False] * n\n",
    "        visited[cycle] = True\n",
    "        # Start from the cycle node and traverse the graph to find all nodes in the cycle\n",
    "        while True:\n",
    "            u = contract[-1]\n",
    "            for _, v, _ in graph:\n",
    "                if not visited[v] and parent[v] == u:\n",
    "                    contract.append(v)\n",
    "                    visited[v] = True\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Step 4: Create a contracted graph by removing edges within the cycle and updating edge weights\n",
    "    contracted_graph = []\n",
    "    for u, v, weight in graph:\n",
    "        if u not in contract and v not in contract:\n",
    "            contracted_graph.append((u, v, weight))\n",
    "        elif u not in contract and v in contract:\n",
    "            contracted_graph.append((u, contract[-1], weight - best_weight[v]))\n",
    "    \n",
    "    # Step 5: Recursively apply Chu-Liu/Edmonds algorithm on the contracted graph\n",
    "    contracted_mst = chu_liu_edmonds(contracted_graph)\n",
    "    \n",
    "    # Step 6: Expand the contracted graph back to the original graph\n",
    "    for u, v, weight in graph:\n",
    "        if v in contract:\n",
    "            if contracted_mst[v] == contract[-1]:\n",
    "                parent[v] = u\n",
    "                best_weight[v] = weight - best_weight[v]\n",
    "    \n",
    "    # Step 7: Find the minimum spanning tree\n",
    "    mst = [(parent[v], v) for v in range(n) if parent[v] != -1]\n",
    "    return mst\n",
    "\n",
    "\n",
    "\n",
    "# Define the sample graph as a list of tuples (u, v, w), representing directed edges from node u to node v with weight w\n",
    "graph = [(0, 1, -1),\n",
    "            (1, 2, 4),\n",
    "            (2, 3, 7),\n",
    "            (3, 0, -2),\n",
    "            (1, 0, 6),\n",
    "            (0, 3, 1),\n",
    "            (3, 2, 5),\n",
    "            (2, 1, 2)]\n",
    "\n",
    "# Call the chu_liu_edmonds function\n",
    "mst = chu_liu_edmonds(graph)\n",
    "# Print the minimum spanning tree\n",
    "print(\"Minimum Spanning Tree:\")\n",
    "for u, v in mst:\n",
    "    print(f\"{u} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd0680",
   "metadata": {},
   "source": [
    "### BLEU Score [Bilingual Evaluation Understudy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5de9642",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_5064/2666028907.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Bablu\\AppData\\Local\\Temp/ipykernel_5064/2666028907.py\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    precision = reference_ngram_count/candidate_ngram_count\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def calculate_bleu_score(candidate, references, n=4):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a given candidate translation and reference translations.\n",
    "    Args:\n",
    "        candidate (list): List of tokens representing the candidate translation\n",
    "        references (list): List of lists of tokens representing the reference translations\n",
    "        n (int): Number of n-grams to use for comparison, default is 4\n",
    "    Returns:\n",
    "        float: BLEU score\n",
    "    \"\"\"\n",
    "    candidate_length = len(candidate)\n",
    "    reference_lengths = [len(ref) for ref in references]\n",
    "    closest_ref_length = min(reference_lengths, key=lambda x: abs(x - candidate_length))\n",
    "    \n",
    "    # Calculate brevity penalty\n",
    "    brevity_penalty = min(1.0, np.exp(1 - closest_ref_length / candidate_length))\n",
    "   \n",
    "    # Calculate n-gram precision scores\n",
    "    ngram_precision_scores = []\n",
    "    for i in range(1, n + 1):\n",
    "        candidate_ngrams = list(nltk.ngrams(candidate, i))\n",
    "        candidate_ngram_freq = nltk.FreqDist(candidate_ngrams)\n",
    "        candidate_ngram_count = sum(candidate_ngram_freq.values())\n",
    "        \n",
    "        reference_ngram_count = 0\n",
    "        for k in candidate_ngram_freq.keys():\n",
    "            reference_ngram_count += min(candidate_ngram_freq[k], max([nltk.FreqDist(nltk.ngrams(reference, i))[k] for reference in references\n",
    "        precision = reference_ngram_count/candidate_ngram_count\n",
    "        ngram_precision_scores.append(precision)\n",
    "        print(f\"Precision for {i}-gram: {precision:.4f}\")\n",
    "                                                               \n",
    "    # Calculate modified n-gram precision\n",
    "    modified_precision = np.exp(np.mean(np.log(ngram_precision_scores)))\n",
    "    print(f\"Modified {n}-gram Precision: {modified_precision:.4f}\")\n",
    "                                                               \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = brevity_penalty * modified_precision\n",
    "    print(f\"Brevity Penalty: {brevity_penalty:.4f}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    return bleu_score\n",
    "                                                           \n",
    "# Example usage\n",
    "candidate1 = ['Mary', 'no', 'slap','the','witch','green']\n",
    "candidate2 = ['Mary', 'did', 'not', 'give','a','smack','to','a','green','witch']\n",
    "\n",
    "references = [['Mary', 'did', 'not', 'slap','the','green','witch'], ['Mary', 'did', 'not', 'smack','the','green','witch'],['Mary', 'did', 'not', 'hit','a','green','sorceress']]\n",
    "n = 2 # n_gram\n",
    "\n",
    "print('BLEU Score of Candidate1:' , calculate_bleu_score(candidate1, references, n))\n",
    "print()\n",
    "print('BLEU Score of Candidate2:' , calculate_bleu_score(candidate2, references, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ad851",
   "metadata": {},
   "source": [
    "### IBM Model 1 with no NULL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece72316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Probabilities:\n",
      "[[nan nan nan]\n",
      " [0.8 0.1 0.1]\n",
      " [nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bablu\\AppData\\Local\\Temp/ipykernel_28876/918744249.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  translation_probs = source_target_word_count / source_word_count.reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_translation_probabilities(source_vocab_size, target_vocab_size):\n",
    "    \"\"\"\n",
    "    Initialize translation probabilities uniformly for all word pairs.\n",
    "    \"\"\"\n",
    "    return np.ones((source_vocab_size, target_vocab_size)) / target_vocab_size\n",
    "\n",
    "def em_algorithm(source_sentences, target_sentences, num_iterations):\n",
    "    \"\"\"\n",
    "    Expectation-Maximization (EM) algorithm for word alignment using IBM Model 1 without NULL generation.\n",
    "    \"\"\"\n",
    "    source_vocab = set()\n",
    "    target_vocab = set()\n",
    "    \n",
    "    # Collect source and target vocabularies\n",
    "    for source_sentence, target_sentence in zip(source_sentences, target_sentences):\n",
    "        source_vocab.update(source_sentence)\n",
    "        target_vocab.update(target_sentence)\n",
    "    \n",
    "    source_vocab_size = len(source_vocab)\n",
    "    target_vocab_size = len(target_vocab)\n",
    "    \n",
    "    # Initialize translation probabilities\n",
    "    translation_probs = initialize_translation_probabilities(source_vocab_size, target_vocab_size)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Initialize counts\n",
    "        source_word_count = np.zeros(source_vocab_size)\n",
    "        source_target_word_count = np.zeros((source_vocab_size, target_vocab_size))\n",
    "        \n",
    "        # Expectation step\n",
    "        for source_sentence, target_sentence in zip(source_sentences, target_sentences):\n",
    "            for source_word in source_sentence:\n",
    "                source_word_idx = list(source_vocab).index(source_word)\n",
    "                total_prob = 0.0\n",
    "                for target_word in target_sentence:\n",
    "                    target_word_idx = list(target_vocab).index(target_word)\n",
    "                    total_prob += translation_probs[source_word_idx][target_word_idx]\n",
    "\n",
    "            for target_word in target_sentence:\n",
    "                target_word_idx = list(target_vocab).index(target_word)\n",
    "                translation_prob = translation_probs[source_word_idx][target_word_idx]\n",
    "                alignment_prob = translation_prob / total_prob\n",
    "                source_target_word_count[source_word_idx][target_word_idx] += alignment_prob\n",
    "                source_word_count[source_word_idx] += alignment_prob\n",
    "        \n",
    "        # Maximization step\n",
    "        translation_probs = source_target_word_count / source_word_count.reshape(-1, 1)\n",
    "    return translation_probs\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "source_sentences = [['green', 'house'],['the', 'house']]\n",
    "\n",
    "target_sentences = [['casa', 'verde'],['la', 'casa']]\n",
    "\n",
    "num_iterations = 3\n",
    "translation_probs = em_algorithm(source_sentences, target_sentences, num_iterations)\n",
    "\n",
    "print(\"Translation Probabilities:\")\n",
    "print(translation_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85662923",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa89bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.5\n"
     ]
    }
   ],
   "source": [
    "def calculate_mrr(results):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR) for a list of ranked results.\n",
    "    Args:\n",
    "    results (list): A list of ranked results, where the correct answer is expected to be found.\n",
    "    Returns:\n",
    "    float: MRR score.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for i, result in enumerate(results):\n",
    "        if result == 1:\n",
    "            # If correct answer is found at rank i+1 (0-based index)\n",
    "            mrr = 1 / (i + 1)\n",
    "            break\n",
    "    return mrr\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Results list, where 1 indicates the correct answer and 0 indicates incorrect answers\n",
    "results = [0, 1, 0, 0, 1, 0, 1, 0, 0, 0]\n",
    "# Calculate MRR\n",
    "mrr = calculate_mrr(results)\n",
    "# Print MRR score\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69d42d",
   "metadata": {},
   "source": [
    "### Using Penn Tree bank, find the POS tag sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5015cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading treebank: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag sequence for sentence 1:\n",
      "[('The', 'DT'), ('actor', 'NN'), ('was', 'VBD'), ('happy', 'JJ'), ('he', 'PRP'), ('got', 'VBD'), ('a', 'DT'), ('part', 'NN'), ('in', 'IN'), ('a', 'DT'), ('movie', 'NN'), ('even', 'RB'), ('though', 'IN'), ('the', 'DT'), ('part', 'NN'), ('was', 'VBD'), ('small', 'JJ'), ('.', '.')]\n",
      "\n",
      "POS tag sequence for sentence 2:\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('full', 'JJ'), ('of', 'IN'), ('ambition', 'NN'), ('and', 'CC'), ('hope', 'NN'), ('and', 'CC'), ('charm', 'NN'), ('of', 'IN'), ('life', 'NN'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('can', 'MD'), ('renounce', 'VB'), ('everything', 'NN'), ('at', 'IN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('need', 'NN'), ('.', '.')]\n",
      "\n",
      "POS tag sequence for sentence 3:\n",
      "[('When', 'WRB'), ('the', 'DT'), ('going', 'VBG'), ('gets', 'VBZ'), ('tough', 'JJ'), (',', ','), ('the', 'DT'), ('tough', 'JJ'), ('get', 'NN'), ('going', 'VBG'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "# Load the Penn Treebank dataset\n",
    "nltk.download('treebank')\n",
    "sentences = [\n",
    "\"The actor was happy he got a part in a movie even though the part was small.\",\n",
    "\"I am full of ambition and hope and charm of life. But I can renounce everything at the time of need.\",\n",
    "\"When the going gets tough, the tough get going.\"\n",
    "]\n",
    "# Define a function to find POS tag sequence for a given sentence\n",
    "def get_pos_tags(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags\n",
    "# Loop through the sentences and print the POS tag sequence for each sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "    pos_tags = get_pos_tags(sentence)\n",
    "    print(f\"POS tag sequence for sentence {i+1}:\")\n",
    "    print(pos_tags)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c905aa",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be23d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of words given positive class:\n",
      "and: 0.0556\n",
      "aston: 0.0556\n",
      "bad: 0.0000\n",
      "clean: 0.0556\n",
      "condition: 0.0000\n",
      "experience: 0.0556\n",
      "for: 0.0000\n",
      "great: 0.1111\n",
      "helpful: 0.0556\n",
      "horrible: 0.0000\n",
      "hotel: 0.1667\n",
      "is: 0.1111\n",
      "of: 0.0000\n",
      "one: 0.0000\n",
      "overall: 0.0556\n",
      "owner: 0.0556\n",
      "the: 0.1111\n",
      "very: 0.0556\n",
      "was: 0.0556\n",
      "week: 0.0000\n",
      "\n",
      "Probabilities of words given negative class:\n",
      "and: 0.0000\n",
      "aston: 0.0000\n",
      "bad: 0.0769\n",
      "clean: 0.0000\n",
      "condition: 0.0769\n",
      "experience: 0.0769\n",
      "for: 0.0769\n",
      "great: 0.0000\n",
      "helpful: 0.0000\n",
      "horrible: 0.0769\n",
      "hotel: 0.0769\n",
      "is: 0.0000\n",
      "of: 0.0769\n",
      "one: 0.0769\n",
      "overall: 0.0000\n",
      "owner: 0.0000\n",
      "the: 0.1538\n",
      "very: 0.0769\n",
      "was: 0.0769\n",
      "week: 0.0769\n",
      "\n",
      "Prediction for Document D6: Positive\n",
      "Probability of positive: 0.0001\n",
      "Probability of negative: 0.0000\n",
      "\n",
      "Prediction for Document D7: Negative\n",
      "Probability of positive: 0.0000\n",
      "Probability of negative: 0.0059\n",
      "\n",
      "Prediction for Document D8: Positive\n",
      "Probability of positive: 0.0000\n",
      "Probability of negative: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bablu\\Documents\\TFCert\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "# Define the training data\n",
    "train_data = {\n",
    "\"D1\": \"The hotel is clean and great\",\n",
    "\"D2\": \"The hotel owner is very helpful\",\n",
    "\"D3\": \"Overall Aston Hotel’s experience was great\",\n",
    "\"D4\": \"The condition of the hotel was very bad\",\n",
    "\"D5\": \"A HORRIBLE EXPERIENCE FOR ONE WEEK\"\n",
    "}\n",
    "# Define the labels for the training data\n",
    "train_labels = {\n",
    "\"D1\": \"Positive\",\n",
    "\"D2\": \"Positive\",\n",
    "\"D3\": \"Positive\",\n",
    "\"D4\": \"Negative\",\n",
    "\"D5\": \"Negative\"\n",
    "}\n",
    "\n",
    "# Create a CountVectorizer to convert text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "# Transform the training data into feature vectors\n",
    "X_train = vectorizer.fit_transform(train_data.values())\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# Get the indices of positive and negative samples\n",
    "pos_indices = np.where(np.array(list(train_labels.values())) == \"Positive\")[0]\n",
    "neg_indices = np.where(np.array(list(train_labels.values())) == \"Negative\")[0]\n",
    "# Calculate the counts of each word given the positive and negative classes\n",
    "pos_word_counts = X_train[pos_indices].sum(axis=0)\n",
    "neg_word_counts = X_train[neg_indices].sum(axis=0)\n",
    "# Calculate the probabilities of each word given the positive and negative classes\n",
    "pos_word_probs = pos_word_counts / pos_word_counts.sum()\n",
    "neg_word_probs = neg_word_counts / neg_word_counts.sum()\n",
    "# Print the probabilities of each word given the positive and negative classes\n",
    "print(\"Probabilities of words given positive class:\")\n",
    "\n",
    "for i, word in enumerate(feature_names):\n",
    "    print(f\"{word}: {pos_word_probs[0, i]:.4f}\")\n",
    "print(\"\\nProbabilities of words given negative class:\")\n",
    "for i, word in enumerate(feature_names):\n",
    "    print(f\"{word}: {neg_word_probs[0, i]:.4f}\")\n",
    "    \n",
    "# Define the test data\n",
    "test_data = {\n",
    "    \"D6\": \"The hotel view was great\",\n",
    "    \"D7\": \"My holiday experience stay in usa so horrible\",\n",
    "    \"D8\": \"Overall the hotel in aston very clean and great\"\n",
    "}\n",
    "\n",
    "# Calculate the probabilities for sentences D6 and D7\n",
    "for doc_id, doc_text in test_data.items():\n",
    "    words = doc_text.lower().split()\n",
    "    pos_prob = 1.0\n",
    "    neg_prob = 1.0\n",
    "    for word in words:\n",
    "        if word in feature_names:\n",
    "            word_idx = feature_names.index(word)\n",
    "            pos_prob *= pos_word_probs[0, word_idx]\n",
    "            neg_prob *= neg_word_probs[0, word_idx]\n",
    "    if pos_prob > neg_prob:\n",
    "        result = \"Positive\"\n",
    "    else:\n",
    "        result = \"Negative\"\n",
    "    print(f\"\\nPrediction for Document {doc_id}: {result}\")\n",
    "    print(f\"Probability of positive: {pos_prob:.4f}\")\n",
    "    print(f\"Probability of negative: {neg_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce6c4b",
   "metadata": {},
   "source": [
    "### TF & IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e22313ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tTF\t\tIDF\t\tTF-IDF\n",
      "--------------------------------------------------\n",
      "Oasis\t\t0.1250\t\t3.2189\t\t0.4024\n",
      "Place\t\t0.2500\t\t1.0498\t\t0.2625\n",
      "Desert\t\t0.1250\t\t2.5257\t\t0.3157\n",
      "Water\t\t0.1250\t\t2.5257\t\t0.3157\n",
      "Comes\t\t0.1250\t\t2.5257\t\t0.3157\n",
      "Beneath\t\t0.1250\t\t3.9120\t\t0.4890\n",
      "Ground\t\t0.1250\t\t2.4079\t\t0.3010\n",
      "Place\t\t0.2500\t\t1.0498\t\t0.2625\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the document and its words with their corresponding occurrence in documents\n",
    "document = \"Oasis Place Desert Water Comes Beneath Ground Place\"\n",
    "words = [\"Oasis\", \"Place\", \"Desert\", \"Water\", \"Comes\", \"Beneath\", \"Ground\", \"Place\"]\n",
    "occurrences = [400, 3500, 800, 800, 800, 200, 900, 3500]\n",
    "# Calculate total number of documents in the collection\n",
    "total_documents = 10000\n",
    "# Calculate Term Frequency (TF) for each word in the document\n",
    "tf = {}\n",
    "\n",
    "for word in words:\n",
    "    tf[word] = document.split().count(word) / len(document.split())\n",
    "    \n",
    "# Calculate Inverse Document Frequency (IDF) for each word\n",
    "idf = {}\n",
    "for word, occ in zip(words, occurrences):\n",
    "    idf[word] = math.log(total_documents / occ)\n",
    "    \n",
    "# Calculate TF-IDF for each word in the document\n",
    "tf_idf = {}\n",
    "for word in words:\n",
    "    tf_idf[word] = tf[word] * idf[word]\n",
    "    \n",
    "# Display the results in a tabulated output\n",
    "print(\"Word\\t\\tTF\\t\\tIDF\\t\\tTF-IDF\")\n",
    "print(\"--------------------------------------------------\")\n",
    "for word in words:\n",
    "    print(f\"{word}\\t\\t{tf[word]:.4f}\\t\\t{idf[word]:.4f}\\t\\t{tf_idf[word]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b345d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BOW) table:\n",
      "   american  best  burger  chinese  dosa  enjoys  indian  manchurian  \\\n",
      "0         1     2       1        0     0       1       0           0   \n",
      "1         0     1       0        0     1       1       1           0   \n",
      "2         0     1       0        1     0       1       0           1   \n",
      "3         0     2       0        0     0       0       1           0   \n",
      "\n",
      "   restaurant  the  \n",
      "0           1    2  \n",
      "1           1    1  \n",
      "2           1    1  \n",
      "3           1    2  \n",
      "\n",
      "Document 1 - the best American restaurant enjoys the best burger\n",
      "         Word  TF       IDF    TF-IDF\n",
      "0    american   1  1.386294  0.628947\n",
      "1        best   2  0.000000  0.000000\n",
      "2      burger   1  1.386294  0.628947\n",
      "3     chinese   0  1.386294  0.000000\n",
      "4        dosa   0  1.386294  0.000000\n",
      "5      enjoys   1  0.287682  0.083308\n",
      "6      indian   0  0.693147  0.000000\n",
      "7  manchurian   0  1.386294  0.000000\n",
      "8  restaurant   1  0.000000  0.000000\n",
      "9         the   2  0.000000  0.000000\n",
      "\n",
      "Document 2 - Indian restaurant enjoys the best dosa\n",
      "         Word  TF       IDF    TF-IDF\n",
      "0    american   0  1.386294  0.000000\n",
      "1        best   1  0.000000  0.000000\n",
      "2      burger   0  1.386294  0.000000\n",
      "3     chinese   0  1.386294  0.000000\n",
      "4        dosa   1  1.386294  0.821753\n",
      "5      enjoys   1  0.287682  0.108847\n",
      "6      indian   1  0.693147  0.323940\n",
      "7  manchurian   0  1.386294  0.000000\n",
      "8  restaurant   1  0.000000  0.000000\n",
      "9         the   1  0.000000  0.000000\n",
      "\n",
      "Document 3 - Chinese restaurant enjoys the best Manchurian\n",
      "         Word  TF       IDF    TF-IDF\n",
      "0    american   0  1.386294  0.000000\n",
      "1        best   1  0.000000  0.000000\n",
      "2      burger   0  1.386294  0.000000\n",
      "3     chinese   1  1.386294  0.772028\n",
      "4        dosa   0  1.386294  0.000000\n",
      "5      enjoys   1  0.287682  0.102260\n",
      "6      indian   0  0.693147  0.000000\n",
      "7  manchurian   1  1.386294  0.772028\n",
      "8  restaurant   1  0.000000  0.000000\n",
      "9         the   1  0.000000  0.000000\n",
      "\n",
      "Document 4 - the best the best Indian restaurant\n",
      "         Word  TF       IDF    TF-IDF\n",
      "0    american   0  1.386294  0.000000\n",
      "1        best   2  0.000000  0.000000\n",
      "2      burger   0  1.386294  0.000000\n",
      "3     chinese   0  1.386294  0.000000\n",
      "4        dosa   0  1.386294  0.000000\n",
      "5      enjoys   0  0.287682  0.000000\n",
      "6      indian   1  0.693147  0.311771\n",
      "7  manchurian   0  1.386294  0.000000\n",
      "8  restaurant   1  0.000000  0.000000\n",
      "9         the   2  0.000000  0.000000\n",
      "\n",
      "TF-IDF table:\n",
      "   american  best    burger   chinese      dosa    enjoys    indian  \\\n",
      "0  0.628947   0.0  0.628947  0.000000  0.000000  0.083308  0.000000   \n",
      "1  0.000000   0.0  0.000000  0.000000  0.821753  0.108847  0.323940   \n",
      "2  0.000000   0.0  0.000000  0.772028  0.000000  0.102260  0.000000   \n",
      "3  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.311771   \n",
      "\n",
      "   manchurian  restaurant  the  \n",
      "0    0.000000         0.0  0.0  \n",
      "1    0.000000         0.0  0.0  \n",
      "2    0.772028         0.0  0.0  \n",
      "3    0.000000         0.0  0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from math import log\n",
    "\n",
    "# Define the documents\n",
    "documents = ['the best American restaurant enjoys the best burger',\n",
    "'Indian restaurant enjoys the best dosa',\n",
    "'Chinese restaurant enjoys the best Manchurian',\n",
    "'the best the best Indian restaurant']\n",
    "\n",
    "# Create a CountVectorizer object for BOW\n",
    "vectorizer_bow = CountVectorizer()\n",
    "# Create a TfidfVectorizer object for TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "# Compute BOW and TF-IDF vectors\n",
    "bow_matrix = vectorizer_bow.fit_transform(documents)\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(documents)\n",
    "# Create BOW dataframe\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "# Create TF-IDF dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "# Compute IDF values for TF-IDF\n",
    "idf_values = [log(len(documents) / sum(tfidf_df[word] > 0)) for word in tfidf_df.columns]\n",
    "# Assign IDF values to TF-IDF dataframe\n",
    "tfidf_df = tfidf_df * idf_values\n",
    "# Print BOW table\n",
    "print(\"Bag of Words (BOW) table:\")\n",
    "print(bow_df)\n",
    "\n",
    "# Print document-wise table for Word, TF, IDF, and TF-IDF values\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"\\nDocument {i + 1} - {document}\")\n",
    "    df = pd.DataFrame({'Word': vectorizer_tfidf.get_feature_names_out(),\n",
    "                        'TF': bow_df.iloc[i].values,\n",
    "                        'IDF': idf_values,\n",
    "                        'TF-IDF': tfidf_df.iloc[i].values})\n",
    "    print(df)\n",
    "\n",
    "# Print TF-IDF table\n",
    "print(\"\\nTF-IDF table:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "430019d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Probabilities for Positive Sentiment:\n",
      "P(Enjoy/Positive) = 0.21428571428571427\n",
      "P(Poor/Positive) = 0.07142857142857142\n",
      "P(Great/Positive) = 0.2857142857142857\n",
      "P(Lovely/Positive) = 0.14285714285714285\n",
      "P(Amazing/Positive) = 0.14285714285714285\n",
      "P(Rude/Positive) = 0.07142857142857142\n",
      "P(Unpleasant/Positive) = 0.07142857142857142\n",
      "\n",
      "Word Probabilities for Negative Sentiment:\n",
      "P(Enjoy/Negative) = 0.08333333333333333\n",
      "P(Poor/Negative) = 0.25\n",
      "P(Great/Negative) = 0.16666666666666666\n",
      "P(Lovely/Negative) = 0.08333333333333333\n",
      "P(Amazing/Negative) = 0.08333333333333333\n",
      "P(Rude/Negative) = 0.16666666666666666\n",
      "P(Unpleasant/Negative) = 0.16666666666666666\n",
      "\n",
      "Conditional Probabilities for Positive and Negative Sentiment given 'Great' and 'Amazing':\n",
      "P(Positive/Great, Amazing) = 0.02040816326530612\n",
      "P(Negative/Great, Amazing) = 0.004629629629629629\n",
      "\n",
      "Document | Sentiment Words | Polarity\n",
      "----------------------------------------\n",
      "D1        | Great, Enjoy, Great | Positive\n",
      "D2        | Poor, Unpleasant | Negative\n",
      "D3        | Enjoy, Amazing   | Positive\n",
      "D4        | Great, Lovely    | Positive\n",
      "D5        | Great, Poor, Rude | Negative\n",
      "D6        | Great, Amazing   | Positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given documents and their sentiment polarities\n",
    "documents = {\n",
    "\"D1\": {\"words\": [\"Great\", \"Enjoy\", \"Great\"], \"polarity\": \"Positive\"},\n",
    "\"D2\": {\"words\": [\"Poor\", \"Unpleasant\"], \"polarity\": \"Negative\"},\n",
    "\"D3\": {\"words\": [\"Enjoy\", \"Amazing\"], \"polarity\": \"Positive\"},\n",
    "\"D4\": {\"words\": [\"Great\", \"Lovely\"], \"polarity\": \"Positive\"},\n",
    "\"D5\": {\"words\": [\"Great\", \"Poor\", \"Rude\"], \"polarity\": \"Negative\"},\n",
    "\"D6\": {\"words\": [\"Great\", \"Amazing\"], \"polarity\": None} # polarity to be determined\n",
    "}\n",
    "\n",
    "# Step 1: Preprocessing the Data\n",
    "vocabulary = list(set(word for doc in documents.values() for word in doc[\"words\"]))\n",
    "\n",
    "# Step 2: Calculate Class Probabilities\n",
    "num_docs = len(documents)\n",
    "num_positive_docs = sum(1 for doc in documents.values() if doc[\"polarity\"] == \"Positive\")\n",
    "num_negative_docs = sum(1 for doc in documents.values() if doc[\"polarity\"] == \"Negative\")\n",
    "p_positive = num_positive_docs / num_docs\n",
    "p_negative = num_negative_docs / num_docs\n",
    "\n",
    "# Step 3: Calculate Word Probabilities with Add-1 Smoothing\n",
    "vocabulary_size = len(vocabulary)\n",
    "word_probs_positive = {word: 0 for word in vocabulary}\n",
    "word_probs_negative = {word: 0 for word in vocabulary}\n",
    "total_words_positive = 0\n",
    "total_words_negative = 0\n",
    "\n",
    "for doc in documents.values():\n",
    "    words = doc[\"words\"]\n",
    "    polarity = doc[\"polarity\"]\n",
    "    if polarity == \"Positive\":\n",
    "        total_words_positive += len(words)\n",
    "        for word in words:\n",
    "            word_probs_positive[word] += 1\n",
    "    elif polarity == \"Negative\":\n",
    "        total_words_negative += len(words)\n",
    "        for word in words:\n",
    "            word_probs_negative[word] += 1\n",
    "\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_probs_positive[word] = (word_probs_positive[word] + 1) / (total_words_positive + vocabulary_size)\n",
    "    word_probs_negative[word] = (word_probs_negative[word] + 1) / (total_words_negative + vocabulary_size)\n",
    "\n",
    "\n",
    "# Step 4: Calculate Document Probabilities\n",
    "document_probs = {}\n",
    "for doc_id, doc in documents.items():\n",
    "    words = doc[\"words\"]\n",
    "    p_doc_positive = p_positive\n",
    "    p_doc_negative = p_negative\n",
    "    for word in words:\n",
    "        p_doc_positive *= word_probs_positive[word]\n",
    "        p_doc_negative *= word_probs_negative[word]\n",
    "    document_probs[doc_id] = {\"positive\": p_doc_positive, \"negative\": p_doc_negative}\n",
    "\n",
    "\n",
    "# Step 5: Determine Sentiment Polarity for Document D6\n",
    "d6_polarity = \"Positive\" if document_probs[\"D6\"][\"positive\"] > document_probs[\"D6\"][\"negative\"] else \"Negative\"\n",
    "documents[\"D6\"][\"polarity\"] = d6_polarity\n",
    "\n",
    "# Print word probabilities for Positive and Negative sentiment\n",
    "print(\"\\nWord Probabilities for Positive Sentiment:\")\n",
    "\n",
    "for word in vocabulary:\n",
    "    print(f\"P({word}/Positive) = {word_probs_positive[word]}\")\n",
    "    \n",
    "print(\"\\nWord Probabilities for Negative Sentiment:\")\n",
    "\n",
    "for word in vocabulary:\n",
    "    print(f\"P({word}/Negative) = {word_probs_negative[word]}\")\n",
    "    \n",
    "# Calculate conditional probabilities for Positive and Negative sentiment given \"Great\" and \"Amazing\"\n",
    "p_positive_great_amazing = word_probs_positive[\"Great\"] * word_probs_positive[\"Amazing\"] * p_positive\n",
    "p_negative_great_amazing = word_probs_negative[\"Great\"] * word_probs_negative[\"Amazing\"] * p_negative\n",
    "\n",
    "# Print conditional probabilities for Positive and Negative sentiment given \"Great\" and \"Amazing\"\n",
    "print(\"\\nConditional Probabilities for Positive and Negative Sentiment given 'Great' and 'Amazing':\")\n",
    "print(f\"P(Positive/Great, Amazing) = {p_positive_great_amazing}\")\n",
    "print(f\"P(Negative/Great, Amazing) = {p_negative_great_amazing}\")\n",
    "# Print the results\n",
    "print()\n",
    "print(\"Document | Sentiment Words | Polarity\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for doc_id, doc in documents.items():\n",
    "    words = \", \".join(doc[\"words\"])\n",
    "    polarity = doc[\"polarity\"]\n",
    "    print(f\"{doc_id:<9} | {words:<16} | {polarity:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e0294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFCert",
   "language": "python",
   "name": "tfcert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
